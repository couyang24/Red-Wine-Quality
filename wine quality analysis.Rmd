---
title: 'Wine Quality Analysis'
author: "Chengran (Owen) Ouyang"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---
******
# Introduction
******

**Objectives:** The goal of this kernel is to find the best approach to identify the quality of the wine. We will go through the basic EDA and visually identify the quality via a 3D interactive plot. 
Moreover, I also applied multiple ML models to make the prediction respectively. Each of the models would have its own strength.


**Rpart** can give you a nice decision tree plot so you can see the variable more intuitively.


**Random Forest** is the model most of the time you can run directly with minimum amount of tuning.


**xgboost** is expected to produce the best result but needs a bit of tuning.


**svm** is an alternative approach and usually give a less correlated result.


**h2o - deeplearning** is one of the easiest tool to apply deep learning model. I could potentially use keras but due to the size and the structure of data. I don't believe deep learning model would outperform xgboost in this case.


Confusion Matrix is used to evaluate the results.


If you have any question, please leave a comment and if you like the kernel, please give me an upvote~ Thanks!


******
# Basic Set up{.tabset .tabset-fade .tabset-pills}
******


******
## Load Packages
******

```{r  message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, skimr, GGally, plotly, viridis, caret, randomForest, e1071, rpart, xgboost, h2o, corrplot)
#pacman::p_load(tidyverse, skimr, GGally, corrplot, plotly, viridis, caret, randomForest, e1071, rpart, rattle, xgboost, h2o)
```

******
## Load Dataset
******

```{r  message=FALSE, warning=FALSE}
wine <- read_csv("winequality-red.csv")
```

******
# EDA{.tabset .tabset-fade .tabset-pills}
******

******
## First Glimpse via skim
******

```{r  message=FALSE, warning=FALSE}
wine %>% skim() %>% kable()
```

******
## Second Glimpse via Corrplot
******


```{r  message=FALSE, warning=FALSE}
wine %>% cor() %>% corrplot.mixed(upper = "ellipse", tl.cex=.8, tl.pos = 'lt', number.cex = .8)
```

******
# Preprocess
******


```{r  message=FALSE, warning=FALSE}
colnames(wine) <- wine %>% colnames() %>% str_replace_all(" ","_")
wine$quality <- as.factor(wine$quality)
```

******
# GGally - ggpairs
******

I have had a quick look and found the following variables: residual\_sugar, free\_sulfur\_dioxide, total\_sulfur\_dioxide, and chlorides do not have significant different across different quality. 
Therefore, these variables are not included in the ggpairs model. Further, I found volatile_acidity, sulphates, and alcohol have more significate different across different quality based on the graph below. 


```{r  message=FALSE, warning=FALSE}
wine %>% 
  mutate(quality = as.factor(quality)) %>% 
  select(-c(residual_sugar, free_sulfur_dioxide, total_sulfur_dioxide, chlorides)) %>% 
  ggpairs(aes(color = quality,alpha=0.4),
          columns=1:7,
          lower=list(continuous="points"),
          upper=list(continuous="blank"),
          axisLabels="none", switch="both")
```

******
# Ployly 3D Interactive Graph
******

```{r  message=FALSE, warning=FALSE}
wine %>% 
  plot_ly(x=~alcohol,y=~volatile_acidity,z= ~sulphates, color=~quality, hoverinfo = 'text', colors = viridis(3),
          text = ~paste('Quality:', quality,
                        '<br>Alcohol:', alcohol,
                        '<br>Volatile Acidity:', volatile_acidity,
                        '<br>sulphates:', sulphates)) %>% 
  add_markers(opacity = 0.8) %>%
  layout(title = "3D Wine Quality",
         annotations=list(yref='paper',xref="paper",y=1.05,x=1.1, text="quality",showarrow=F),
         scene = list(xaxis = list(title = 'Alcohol'),
                      yaxis = list(title = 'Volatile Acidity'),
                      zaxis = list(title = 'sulphates')))
```

******
# Cross Validation Setup
******

```{r  message=FALSE, warning=FALSE}
set.seed(1)
inTrain <- createDataPartition(wine$quality, p=.9, list = F)

train <- wine[inTrain,]
valid <- wine[-inTrain,]
rm(inTrain)
```

******
# Decision Tree via rpart
******

```{r  message=FALSE, warning=FALSE}
# rpart
set.seed(1)
rpart_model <- rpart(quality~alcohol+volatile_acidity+citric_acid+
                   density+pH+sulphates, train)

#fancyRpartPlot(rpart_model)

rpart_result <- predict(rpart_model, newdata = valid[,!colnames(valid) %in% c("quality")],type='class')

confusionMatrix(valid$quality,rpart_result)
rm(rpart_model, rpart_result)
```

******
# Random Forest
******

```{r  message=FALSE, warning=FALSE}
# randomforest
set.seed(1)
rf_model <- randomForest(quality~alcohol+volatile_acidity+citric_acid+
                           density+pH+sulphates,train)
rf_result <- predict(rf_model, newdata = valid[,!colnames(valid) %in% c("quality")])

confusionMatrix(valid$quality,rf_result)
rm(rf_model, rf_result)
```


******
# SVM
******

```{r  message=FALSE, warning=FALSE}
# svm
set.seed(1)
svm_model <- svm(quality~alcohol+volatile_acidity+citric_acid+
                           density+pH+sulphates,train)
svm_result <- predict(svm_model, newdata = valid[,!colnames(valid) %in% c("quality")])

confusionMatrix(valid$quality,svm_result)
rm(svm_model, svm_result)
```


******
# xgboost
******

```{r  message=FALSE, warning=FALSE}
# xgboost
data.train <- xgb.DMatrix(data = data.matrix(train[, !colnames(valid) %in% c("quality")]), label = train$quality)
data.valid <- xgb.DMatrix(data = data.matrix(valid[, !colnames(valid) %in% c("quality")]))


parameters <- list(
  # General Parameters
  booster            = "gbtree",          # default = "gbtree"
  silent             = 0,                 # default = 0
  # Booster Parameters
  eta                = 0.2,               # default = 0.2, range: [0,1]
  gamma              = 0,                 # default = 0,   range: [0,???]
  max_depth          = 5,                 # default = 5,   range: [1,???]
  min_child_weight   = 2,                 # default = 2,   range: [0,???]
  subsample          = 1,                 # default = 1,   range: (0,1]
  colsample_bytree   = 1,                 # default = 1,   range: (0,1]
  colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
  lambda             = 1,                 # default = 1
  alpha              = 0,                 # default = 0
  # Task Parameters
  objective          = "multi:softmax",   # default = "reg:linear"
  eval_metric        = "merror",
  num_class          = 7,
  seed               = 1               # reproducability seed
)

xgb_model <- xgb.train(parameters, data.train, nrounds = 100)

xgb_pred <- predict(xgb_model, data.valid)

confusionMatrix(as.factor(xgb_pred+2), valid$quality)
rm(xgb_model, xgb_pred, data.train, data.valid, parameters)
```


******
# h2o (deeplearning)
******

```{r  message=FALSE, warning=FALSE}
# h2o
h2o.init()
h2o.train <- as.h2o(train)
h2o.valid <- as.h2o(valid)


h2o.model <- h2o.deeplearning(x = setdiff(names(train), c("quality")),
                              y = "quality",
                              training_frame = h2o.train,
                              # activation = "RectifierWithDropout", # algorithm
                              # input_dropout_ratio = 0.2, # % of inputs dropout
                              # balance_classes = T,
                              # momentum_stable = 0.99,
                              # nesterov_accelerated_gradient = T, # use it for speed
                              epochs = 1000,
                              standardize = TRUE,         # standardize data
                              hidden = c(100, 100),       # 2 layers of 00 nodes each
                              rate = 0.05,                # learning rate
                              seed = 1                # reproducability seed
)

h2o.predictions <- h2o.predict(h2o.model, h2o.valid) %>% as.data.frame()

confusionMatrix(h2o.predictions$predict, valid$quality)
rm(h2o.model, h2o.train, h2o.valid, h2o.predictions)
```

******
# Conclusion
******

As I expected, xgboost give the best outcome but an ensemble model might potential give improved result.